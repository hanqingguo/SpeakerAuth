training: !!bool "False"
loss_func: "ge2e"  # ge2e, te2e, match
device: "cuda"
visible: "0"
model_name: 'glg'  # glg, vgg, resnet50, xvct, etdnn, dtdnn, aert, ecapa, ftdnn, resnet34
unprocessed_data: '/data/TIMIT/*/*/*/*.WAV'
---
data:
    train_path: "./train_data/"
    test_path: "./test_data/"
    sr: 16000
    nfft: 512 #For mel spectrogram preprocess
    window: 0.025 #(s)
    hop: 0.01 #(s)
    nmels: 40 #Number of mel energies
    tisv_frame: 180 #Max number of time steps in input after preprocess
---   
model:
    hidden: 768 #Number of LSTM hidden layer units
    num_layer: 3 #Number of LSTM layers
    proj: 256 #Embedding size
    model_path: './checkpoints/GE2E/ckpt_epoch_10.pth'
    # model_path: './checkpoints_paper/lstm_baseline_TE2E/ckpt_epoch_2_batch_id_283.pth'

---
train:
    N : 8 #Number of speakers in batch; before is 2
    M : 6 #Number of utterances per speaker
    num_workers: 0 #number of workers for dataloader
    lr: 0.01
    epochs: 10 #Max training speaker epoch
    log_interval: 10 #Epochs before printing progress
    log_file: './checkpoints/GE2E/Stats'
    checkpoint_interval: 10 #Save model after x speaker epochs
    checkpoint_dir: './checkpoints/GE2E'
    restore: !!bool "False" #Resume training from previous model path
---
test:
    N: 4 #Number of speakers in batch
    M: 6 #Number of utterances per speaker
    num_workers: 1 #number of workers for data loader
    epochs: 5 #testing speaker epochs
